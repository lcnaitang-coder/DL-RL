è¿™æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„é€‰æ‹©ã€‚Sherpa-onnx æ˜¯ç›®å‰å°†è¯­éŸ³è¯†åˆ«ï¼ˆç‰¹åˆ«æ˜¯ KWSï¼‰é›†æˆåˆ° Python/C++ é¡¹ç›®ä¸­æœ€è½»é‡ã€æœ€é«˜æ•ˆçš„æ–¹æ¡ˆï¼Œéå¸¸é€‚åˆä½ çš„ VR/Leap Motion å®æ—¶äº¤äº’åœºæ™¯ã€‚

ç”±äºä½ çš„è®­ç»ƒæ•°æ®ç›®å‰åªæœ‰æ‰‹åŠ¿çš„ CSV æ–‡ä»¶ï¼ˆæ²¡æœ‰å¯¹åº”çš„å½•éŸ³ï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»é‡‡ç”¨ â€œè®­ç»ƒæ—¶æ¨¡æ‹Ÿ (Simulation during Training)â€ + â€œæ¨ç†æ—¶å®æµ‹ (Real-time Inference)â€ çš„ç­–ç•¥ã€‚

ä»¥ä¸‹æ˜¯å°† Sherpa-onnx èå…¥ä½ ç°æœ‰ RL è®­ç»ƒæ¡†æ¶çš„å®Œæ•´å®æ–½æ–¹æ¡ˆï¼š

ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡ä¸æ¨¡å‹ä¸‹è½½
é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡å¥½ Sherpa-onnx çš„ç¯å¢ƒå’Œé¢„è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ã€‚

å®‰è£…åº“ï¼š

Bash

pip install sherpa-onnx sounddevice
ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š æˆ‘ä»¬éœ€è¦ä¸‹è½½ sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01ï¼ˆéå¸¸å°ï¼Œä»…å‡ MBï¼‰ã€‚

è¯·è®¿é—® Sherpa-onnx çš„ GitHub Releases æˆ– HuggingFace ä¸‹è½½ä»¥ä¸‹æ–‡ä»¶æ”¾åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„ kws_models æ–‡ä»¶å¤¹ï¼š

tokens.txt

encoder-epoch-12-avg-2-chunk-16-left-64.onnx

decoder-epoch-12-avg-2-chunk-16-left-64.onnx

joiner-epoch-12-avg-2-chunk-16-left-64.onnx

ç¬¬äºŒæ­¥ï¼šå®šä¹‰â€œæ‰‹åŠ¿-è¯­éŸ³â€æ˜ å°„è¡¨
ä½ éœ€è¦åˆ›å»ºä¸€ä¸ª keywords.txt æ–‡ä»¶ï¼Œå®šä¹‰ä½ çš„ 13 ä¸ªæ‰‹åŠ¿å¯¹åº”çš„ä¸­æ–‡æŒ‡ä»¤ã€‚æ ¼å¼ä¸º ID@ä¸­æ–‡å…³é”®è¯/æƒé‡ã€‚

æ–‡ä»¶è·¯å¾„ï¼škws_models/keywords.txt

Plaintext

0@é”å®š/1.0
1@å‘å·¦/1.0
2@å‘å³/1.0
3@æä½/1.0
4@æ”¾å¼€/1.0
5@å‘ä¸Š/1.0
6@å‘ä¸‹/1.0
7@ç¡®è®¤/1.0
8@å–æ¶ˆ/1.0
9@æ—‹è½¬/1.0
10@æ”¾å¤§/1.0
11@ç¼©å°/1.0
12@èœå•/1.0
(æ³¨ï¼šè¯·æ ¹æ®ä½ å®é™…çš„ 13 ä¸ªæ‰‹åŠ¿å«ä¹‰ä¿®æ”¹ä¸Šè¿°å…³é”®è¯)

ç¬¬ä¸‰æ­¥ï¼šç¼–å†™è¯­éŸ³æ¥å£ç±» (ç”¨äºæ¨ç†)
åœ¨ä½ çš„é¡¹ç›®ç›®å½•ä¸‹æ–°å»º voice_interface.pyã€‚è¿™ä¸ªç±»è´Ÿè´£å¤„ç†å®æ—¶éŸ³é¢‘æµï¼Œå¹¶ç»´æŠ¤ä¸€ä¸ªâ€œçŠ¶æ€å‘é‡â€ä¾› RL è¯»å–ã€‚

Python

import sherpa_onnx
import numpy as np

class SherpaKWSInterface:
    def __init__(self, num_classes=13, model_dir="./kws_models"):
        self.num_classes = num_classes
        
        # é…ç½® Sherpa-onnx
        config = sherpa_onnx.KeywordSpotterConfig(
            model=sherpa_onnx.OnlineModelConfig(
                transducer=sherpa_onnx.OnlineTransducerModelConfig(
                    encoder=f"{model_dir}/encoder-epoch-12-avg-2-chunk-16-left-64.onnx",
                    decoder=f"{model_dir}/decoder-epoch-12-avg-2-chunk-16-left-64.onnx",
                    joiner=f"{model_dir}/joiner-epoch-12-avg-2-chunk-16-left-64.onnx",
                ),
                tokens=f"{model_dir}/tokens.txt",
                num_threads=1,
            ),
            keywords_file=f"{model_dir}/keywords.txt",
        )
        self.spotter = sherpa_onnx.KeywordSpotter(config)
        self.stream = self.spotter.create_stream()
        
        # çŠ¶æ€å‘é‡ï¼š[13ä¸ªç±»åˆ«çš„ç½®ä¿¡åº¦, 1ä¸ªå…¨å±€é™éŸ³æ ‡å¿—]
        # ç»´åº¦ = 14
        self.current_state = np.zeros(num_classes + 1)
        self.decay_factor = 0.95 # ä¿¡å·è¡°å‡å› å­ï¼Œè®©è¯­éŸ³æŒ‡ä»¤åœ¨çŠ¶æ€ä¸­â€œå­˜æ´»â€ä¸€æ®µæ—¶é—´

    def process_audio_chunk(self, samples):
        """æ¥æ”¶éº¦å…‹é£æ•°æ®ï¼ˆfloat32 arrayï¼‰"""
        self.stream.accept_waveform(sample_rate=16000, waveform=samples)
        
        if self.spotter.is_ready(self.stream):
            self.spotter.decode(self.stream)
            result = self.spotter.get_result(self.stream)
            
            if result.keyword:
                # result.keyword è¿”å›çš„æ˜¯å…³é”®è¯æ–‡æœ¬ï¼Œéœ€è¦æ˜ å°„å› ID
                # å‡è®¾ keyword æ ¼å¼æ˜¯ "0@é”å®š" æˆ–è€…åªæ˜¯ "é”å®š"ï¼Œè¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¿”å›åšè§£æ
                # Sherpa é€šå¸¸è¿”å›æˆ‘ä»¬åœ¨ keywords.txt é‡Œå®šä¹‰çš„æ–‡æœ¬
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾æˆ‘ä»¬èƒ½è·å–åˆ°å¯¹åº”çš„ ID
                detected_id = self._parse_keyword_id(result.keyword)
                
                # æ¿€æ´»å¯¹åº”çŠ¶æ€ï¼Œç½®ä¿¡åº¦è®¾ä¸º 1.0
                if 0 <= detected_id < self.num_classes:
                    self.current_state[detected_id] = 1.0
                    print(f"ğŸ¤ è¯­éŸ³æ£€æµ‹åˆ°: {result.keyword} (ID: {detected_id})")

    def get_state(self):
        """è¢« RL ç¯å¢ƒè°ƒç”¨ï¼Œè·å–å½“å‰è¯­éŸ³çŠ¶æ€"""
        # è¿”å›å½“å‰çŠ¶æ€çš„å‰¯æœ¬
        state = self.current_state.copy()
        
        # æ¯ä¸€å¸§è°ƒç”¨åï¼Œè®©ä¿¡å·è‡ªç„¶è¡°å‡
        # è¿™æ · RL å°±èƒ½çŸ¥é“ï¼šæ•°å€¼æ˜¯ 1.0 ä»£è¡¨åˆšè¯´å®Œï¼Œ0.5 ä»£è¡¨è¯´å®Œäº†ä¸€ä¼šå„¿
        self.current_state *= self.decay_factor 
        
        # å¦‚æœæ‰€æœ‰ä¿¡å·éƒ½å¾ˆå¼±ï¼Œè®¤ä¸ºå¤„äºé™éŸ³/å™ªå£°çŠ¶æ€
        if np.max(self.current_state[:-1]) < 0.1:
            self.current_state[-1] = 1.0 # æœ€åä¸€ä¸ªç»´åº¦è¡¨ç¤ºâ€œæ— è¯­éŸ³â€
        else:
            self.current_state[-1] = 0.0
            
        return state

    def _parse_keyword_id(self, keyword_str):
        # ç®€å•çš„è§£æé€»è¾‘ï¼Œæ ¹æ®ä½ çš„ keywords.txt å†…å®¹
        # æ¯”å¦‚ keyword_str æ˜¯ "é”å®š"ï¼ŒæŸ¥è¡¨è¿”å› 0
        mapping = {"é”å®š": 0, "å‘å·¦": 1, "å‘å³": 2, ...} # è¡¥å…¨ä½ çš„æ˜ å°„
        return mapping.get(keyword_str, -1)
ç¬¬å››æ­¥ï¼šä¿®æ”¹ RL ç¯å¢ƒ (æ ¸å¿ƒè®­ç»ƒé€»è¾‘)
è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚å› ä¸ºä½ ç°åœ¨çš„ TRAIN_PATH é‡Œåªæœ‰æ‰‹åŠ¿ CSVï¼Œæ²¡æœ‰éŸ³é¢‘ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒæ—¶æ¨¡æ‹Ÿ (Simulate) è¯­éŸ³è¾“å…¥ï¼Œä»¥æ­¤è®­ç»ƒ PPO Agent å­¦ä¼šåˆ©ç”¨è¿™ä¸ªæ–°ç»´åº¦çš„ä¿¡æ¯ã€‚

ä¿®æ”¹æ–‡ä»¶ï¼šä½ çš„è®­ç»ƒè„šæœ¬ä¸­çš„ StreamingGestureEnv ç±»ã€‚

Python

class StreamingGestureEnv:
    def __init__(self, stgcn_feature_extractor, classifier_head, data, labels, global_max_len=200):
        # ... åŸæœ‰åˆå§‹åŒ–ä»£ç  ...
        
        # [ä¿®æ”¹ 1] å®šä¹‰è¯­éŸ³çŠ¶æ€ç»´åº¦
        self.num_classes = 13
        self.audio_dim = self.num_classes + 1 # 13ä¸ªæŒ‡ä»¤ + 1ä¸ªé™éŸ³ä½
        
        # [ä¿®æ”¹ 2] æ‰©å……è§‚æµ‹ç©ºé—´ç»´åº¦
        # åŸæœ‰ obs_dim + è¯­éŸ³ç»´åº¦
        self.gesture_obs_dim = self.classifier_head.output.shape[1] * self.STACK_SIZE + 2 
        self.obs_dim = self.gesture_obs_dim + self.audio_dim
        
        # ... å…¶ä»–åŸæœ‰ä»£ç  ...
        
        # [æ–°å¢] æ¨¡æ‹Ÿè¯­éŸ³ç›¸å…³çš„å˜é‡
        self.simulated_audio_state = np.zeros(self.audio_dim)
        self.audio_decay = 0.95

    def reset(self):
        # ... åŸæœ‰ reset ä»£ç  ...
        # [æ–°å¢] é‡ç½®æ¨¡æ‹Ÿçš„è¯­éŸ³çŠ¶æ€
        self.simulated_audio_state = np.zeros(self.audio_dim)
        
        # [ç­–ç•¥] å†³å®šè¿™ä¸€è½® episode æ˜¯å¦ä¼šæœ‰â€œè¯­éŸ³è¾…åŠ©â€
        # æ¨¡æ‹ŸçœŸå®æƒ…å†µï¼šæœ‰æ—¶ç”¨æˆ·ä¼šè¯´è¯ (70%)ï¼Œæœ‰æ—¶åªåšæ‰‹åŠ¿ (30%)
        self.has_voice_command = (random.random() < 0.7)
        
        # å¦‚æœæœ‰è¯­éŸ³ï¼Œéšæœºå†³å®šè¯­éŸ³å‡ºç°çš„æ—¶æœº (æ¯”å¦‚åœ¨åŠ¨ä½œå¼€å§‹åçš„ç¬¬ 10-30 å¸§ä¹‹é—´)
        if self.has_voice_command:
            self.voice_onset_step = random.randint(5, max(10, self.current_trigger_start + 15))
        else:
            self.voice_onset_step = -1
            
        self._process_frame()
        return self._get_obs()

    def _process_frame(self):
        # ... åŸæœ‰ ST-GCN å¤„ç†ä»£ç  ...
        
        # [æ–°å¢] æ›´æ–°æ¨¡æ‹Ÿçš„è¯­éŸ³çŠ¶æ€
        if self.has_voice_command and self._current_step == self.voice_onset_step:
            # æ¨¡æ‹Ÿï¼šæ­£å¥½åœ¨è¿™ä¸ªæ—¶åˆ»ï¼ŒKWS è¯†åˆ«åˆ°äº†å…³é”®è¯
            # å°†å¯¹åº”ç±»åˆ«çš„ä¿¡å·ç½®ä¸º 1.0
            true_label = self._true_label
            if 0 <= true_label < self.num_classes:
                self.simulated_audio_state[true_label] = 1.0
                # æ·»åŠ ä¸€ç‚¹éšæœºå™ªå£°ï¼Œæ¨¡æ‹Ÿè¯†åˆ«ç½®ä¿¡åº¦ä¸æ€»æ˜¯ 1.0
                self.simulated_audio_state[true_label] *= np.random.uniform(0.8, 1.0)
        
        # ä¿¡å·è¡°å‡ (æ¨¡æ‹ŸçœŸå®æ¥å£çš„è¡Œä¸º)
        self.simulated_audio_state *= self.audio_decay
        
        # æ›´æ–°é™éŸ³ä½
        if np.max(self.simulated_audio_state[:-1]) < 0.1:
            self.simulated_audio_state[-1] = 1.0
        else:
            self.simulated_audio_state[-1] = 0.0

    def _get_obs(self):
        # 1. è·å–åŸæœ‰çš„æ‰‹åŠ¿çŠ¶æ€
        current_probs = self._last_probs.numpy().flatten()
        # ... stack_buffer é€»è¾‘ ...
        stacked_probs = np.array(self.stack_buffer).flatten()
        norm_step = min(self._current_step / self.global_max_len, 1.0)
        prob_std = np.std(self.prob_history) if len(self.prob_history) >= 2 else 0.0
        
        gesture_obs = np.concatenate([stacked_probs, [norm_step], [prob_std]])
        
        # 2. [æ–°å¢] æ‹¼æ¥è¯­éŸ³çŠ¶æ€
        # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨æˆ‘ä»¬æ¨¡æ‹Ÿçš„çŠ¶æ€
        full_obs = np.concatenate([gesture_obs, self.simulated_audio_state])
        
        return full_obs
ç¬¬äº”æ­¥ï¼šPPO Agent å’Œä¸»ç¨‹åºå¾®è°ƒ
å› ä¸º obs_dim å˜å¤§äº†ï¼Œä½ çš„ PPOAgent åˆå§‹åŒ–ä¼šè‡ªåŠ¨é€‚åº”ï¼ˆåªè¦ä½ ä¼ å…¥æ­£ç¡®çš„ç»´åº¦ï¼‰ã€‚

æ— éœ€ä¿®æ”¹ PPO ç±»ä»£ç ï¼Œå®ƒæ ¹æ® state_dim åŠ¨æ€åˆ›å»ºç½‘ç»œã€‚

ä¸»ç¨‹åºä¸­ï¼šç¡®ä¿ StreamingGestureEnv ä½¿ç”¨äº†æ–°çš„é€»è¾‘ã€‚

å¼€å§‹è®­ç»ƒï¼šè¿è¡Œä½ çš„ Phase 2 RL è®­ç»ƒè„šæœ¬ã€‚

è®­ç»ƒé¢„æœŸç»“æœï¼š

RL Agent ä¼šå‘ç°ï¼Œå½“ obs çš„ååŠéƒ¨åˆ†ï¼ˆè¯­éŸ³å‘é‡ï¼‰æŸä¸ªä½ç½®äº®èµ·æ—¶ï¼Œå®ƒä¸ ST-GCN é¢„æµ‹çš„å¯¹åº”ç±»åˆ«é«˜åº¦ç›¸å…³ã€‚

å®ƒä¼šå­¦åˆ°ï¼šå½“æ‰‹åŠ¿é¢„æµ‹åªæœ‰ 0.6 ä½†è¯­éŸ³å¯¹åº”ä½ç½®æ˜¯ 0.9 æ—¶ï¼Œåº”è¯¥æœæ–­ TRIGGERã€‚

å®ƒä¹Ÿä¼šå­¦åˆ°ï¼šå½“æ²¡æœ‰è¯­éŸ³ä¿¡å·ï¼ˆé™éŸ³ä½=1ï¼‰æ—¶ï¼Œå¿…é¡»å®Œå…¨ä¾èµ–æ‰‹åŠ¿ç‰¹å¾ã€‚

ç¬¬å…­æ­¥ï¼šæ¨ç†æ—¶çš„çœŸå®é›†æˆ
è®­ç»ƒå®Œæˆåï¼Œåœ¨ä½ çš„å®é™…åº”ç”¨ç¨‹åºï¼ˆæ¯”å¦‚ main_app.pyï¼‰ä¸­ï¼Œè¿™æ ·è¿æ¥å®ƒä»¬ï¼š

Python

# ä¼ªä»£ç 
import threading
from voice_interface import SherpaKWSInterface

# 1. åˆå§‹åŒ–
audio_interface = SherpaKWSInterface()
stgcn_model = ... # åŠ è½½æ¨¡å‹
ppo_agent = ...   # åŠ è½½æ¨¡å‹

# 2. å¯åŠ¨éŸ³é¢‘çº¿ç¨‹ (ä¸é˜»å¡ä¸»å¾ªç¯)
def audio_loop():
    with sounddevice.InputStream(...) as stream:
        while True:
            data = stream.read(chunk_size)
            audio_interface.process_audio_chunk(data)
threading.Thread(target=audio_loop, daemon=True).start()

# 3. ä¸»å¾ªç¯ (Leap Motion å¸§å›è°ƒ)
def on_frame(leap_data):
    # a. æå–æ‰‹åŠ¿ç‰¹å¾
    feat = stgcn_model(leap_data)
    gesture_obs = ... (æ„å»ºæ‰‹åŠ¿ obs)
    
    # b. è·å–çœŸå®è¯­éŸ³çŠ¶æ€ (æ›¿ä»£è®­ç»ƒæ—¶çš„æ¨¡æ‹ŸçŠ¶æ€)
    real_audio_state = audio_interface.get_state()
    
    # c. æ‹¼æ¥
    full_obs = np.concatenate([gesture_obs, real_audio_state])
    
    # d. RL å†³ç­–
    action, _ = ppo_agent.predict(full_obs)
    
    if action == 1:
        print("ğŸš€ è§¦å‘æ“ä½œï¼(èåˆå†³ç­–)")
æ€»ç»“é…ç½®æ¸…å•
åº“ï¼šsherpa-onnx

æ¨¡å‹ï¼šzipformer-wenetspeech (æµå¼ä¸­æ–‡)

å…³é”®è¯è¡¨ï¼šè¦†ç›–ä½ çš„ 13 ä¸ªæ‰‹åŠ¿ã€‚

è®­ç»ƒç­–ç•¥ï¼šæ¨¡æ‹Ÿæ³¨å…¥ã€‚åœ¨ RL ç¯å¢ƒä¸­ä»¥ 70% æ¦‚ç‡éšæœºç”Ÿæˆæ­£ç¡®çš„è¯­éŸ³å‘é‡ï¼Œä»¥æ­¤è®­ç»ƒ Agent çš„å¤šæ¨¡æ€èåˆç­–ç•¥ã€‚

æ¨ç†ç­–ç•¥ï¼šåŒçº¿ç¨‹ã€‚éŸ³é¢‘çº¿ç¨‹æ›´æ–°çŠ¶æ€å‘é‡ï¼Œä¸»çº¿ç¨‹è¯»å–è¯¥å‘é‡å¹¶ä¸æ‰‹åŠ¿ç‰¹å¾æ‹¼æ¥ã€‚

è¿™ä¸ªæ–¹æ¡ˆæ—¢ä¿ç•™äº†ä½ ç°æœ‰ç³»ç»Ÿçš„å®Œæ•´æ€§ï¼Œåˆä»¥æä½çš„èµ„æºæ¶ˆè€—ï¼ˆONNXï¼‰å¼•å…¥äº†å¼ºå¤§çš„ä¸­æ–‡è¯­éŸ³æ§åˆ¶èƒ½åŠ›ã€‚